{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0vyTaRGOdmB"
      },
      "source": [
        "## Basic Structured Operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2KpLHzVOdmE"
      },
      "source": [
        "### Step 1: Initialize PySpark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Xue9qjMOdmE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/04 15:41:00 WARN Utils: Your hostname, anujs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.68 instead (on interface en0)\n",
            "23/09/04 15:41:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/09/04 15:41:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/04 15:41:14 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
          ]
        }
      ],
      "source": [
        "#Importing the library to initiate the SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"day2\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCeSCVDKOdmG"
      },
      "source": [
        "### Step 2: Load the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QHmqfHtkOdmG"
      },
      "outputs": [],
      "source": [
        "# Load the Chipotle dataset into a Spark DataFrame\n",
        "data_path = \"/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Assignment_2023/Day_2/occupation.csv\"  # Replace with the actual path\n",
        "occupation = spark.read.csv(data_path, header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8629vMtbOdmG",
        "outputId": "6b36b5bf-e280-458d-a728-3e2c6c8e4717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- occupation: string (nullable = true)\n",
            " |-- zip_code: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#To get the schema of the datasets\n",
        "occupation.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS0eyFnbOdmH"
      },
      "source": [
        "### Problem 1: Selecting Specific Columns\n",
        "Problem: Select the \"user_id,\" \"age,\" and \"occupation\" columns from the occupation DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWuoKeF6OdmH",
        "outputId": "df8be87c-480f-4423-ea3a-cafeb9b11249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+-------------+\n",
            "|user_id|age|   occupation|\n",
            "+-------+---+-------------+\n",
            "|      1| 24|   technician|\n",
            "|      2| 53|        other|\n",
            "|      3| 23|       writer|\n",
            "|      4| 24|   technician|\n",
            "|      5| 33|        other|\n",
            "|      6| 42|    executive|\n",
            "|      7| 57|administrator|\n",
            "|      8| 36|administrator|\n",
            "|      9| 29|      student|\n",
            "|     10| 53|       lawyer|\n",
            "|     11| 39|        other|\n",
            "|     12| 28|        other|\n",
            "|     13| 47|     educator|\n",
            "|     14| 45|    scientist|\n",
            "|     15| 49|     educator|\n",
            "|     16| 21|entertainment|\n",
            "|     17| 30|   programmer|\n",
            "|     18| 35|        other|\n",
            "|     19| 40|    librarian|\n",
            "|     20| 42|    homemaker|\n",
            "+-------+---+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Selectiong 'user_id' 'age' and 'occupation' from the dataframe\n",
        "\n",
        "q1 = occupation.select('user_id',\"age\",\"occupation\")\n",
        "q1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU96BgfUOdmI"
      },
      "source": [
        "### Problem 2: Filtering Rows based on Condition\n",
        "Problem: Find the users who are older than 30 years from the occupation DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT23c29oOdmI",
        "outputId": "3695d396-a028-4683-f2d6-4aa280ef598b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+------+-------------+--------+\n",
            "|user_id|age|gender|   occupation|zip_code|\n",
            "+-------+---+------+-------------+--------+\n",
            "|      2| 53|     F|        other|   94043|\n",
            "|      5| 33|     F|        other|   15213|\n",
            "|      6| 42|     M|    executive|   98101|\n",
            "|      7| 57|     M|administrator|   91344|\n",
            "|      8| 36|     M|administrator|   05201|\n",
            "|     10| 53|     M|       lawyer|   90703|\n",
            "|     11| 39|     F|        other|   30329|\n",
            "|     13| 47|     M|     educator|   29206|\n",
            "|     14| 45|     M|    scientist|   55106|\n",
            "|     15| 49|     F|     educator|   97301|\n",
            "|     18| 35|     F|        other|   37212|\n",
            "|     19| 40|     M|    librarian|   02138|\n",
            "|     20| 42|     F|    homemaker|   95660|\n",
            "|     25| 39|     M|     engineer|   55107|\n",
            "|     26| 49|     M|     engineer|   21044|\n",
            "|     27| 40|     F|    librarian|   30030|\n",
            "|     28| 32|     M|       writer|   55369|\n",
            "|     29| 41|     M|   programmer|   94043|\n",
            "|     34| 38|     F|administrator|   42141|\n",
            "|     39| 41|     M|entertainment|   01040|\n",
            "+-------+---+------+-------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Finding the users older than 30 years\n",
        "\n",
        "older_than_30 = occupation.filter(occupation.age>30)\n",
        "older_than_30.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UgD4jOROdmI"
      },
      "source": [
        "### Problem 3: Counting and Grouping\n",
        "Problem: Count the number of users in each occupation from the occupation DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQc6oqHHOdmI",
        "outputId": "80537520-1c0a-4f1c-808a-fea0f00bd820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+\n",
            "|   occupation|user_count|\n",
            "+-------------+----------+\n",
            "|    librarian|        51|\n",
            "|      retired|        14|\n",
            "|       lawyer|        12|\n",
            "|         none|         9|\n",
            "|       writer|        45|\n",
            "|   programmer|        66|\n",
            "|    marketing|        26|\n",
            "|        other|       105|\n",
            "|    executive|        32|\n",
            "|    scientist|        31|\n",
            "|      student|       196|\n",
            "|     salesman|        12|\n",
            "|       artist|        28|\n",
            "|   technician|        27|\n",
            "|administrator|        79|\n",
            "|     engineer|        67|\n",
            "|   healthcare|        16|\n",
            "|     educator|        95|\n",
            "|entertainment|        18|\n",
            "|    homemaker|         7|\n",
            "+-------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Counting the no. of users in each occupation from occupation dataframe\n",
        "occupation.groupBy(\"occupation\").count().withColumnRenamed(\"count\",\"user_count\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_hay_fTOdmJ"
      },
      "source": [
        "### Problem 4: Adding a New Column\n",
        "Problem: Add a new column \"age_group\" to the occupation DataFrame based on the age of the users. Divide users into age groups: \"18-25\", \"26-35\", \"36-50\", and \"51+\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4DTh0q7OdmJ",
        "outputId": "e723bbad-f1a5-4d5a-e1d1-84536e1aafe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+------+-------------+--------+---------+\n",
            "|user_id|age|gender|   occupation|zip_code|age_group|\n",
            "+-------+---+------+-------------+--------+---------+\n",
            "|      1| 24|     M|   technician|   85711|    18-25|\n",
            "|      2| 53|     F|        other|   94043|      51+|\n",
            "|      3| 23|     M|       writer|   32067|    18-25|\n",
            "|      4| 24|     M|   technician|   43537|    18-25|\n",
            "|      5| 33|     F|        other|   15213|    26-35|\n",
            "|      6| 42|     M|    executive|   98101|    36-50|\n",
            "|      7| 57|     M|administrator|   91344|      51+|\n",
            "|      8| 36|     M|administrator|   05201|    36-50|\n",
            "|      9| 29|     M|      student|   01002|    26-35|\n",
            "|     10| 53|     M|       lawyer|   90703|      51+|\n",
            "|     11| 39|     F|        other|   30329|    36-50|\n",
            "|     12| 28|     F|        other|   06405|    26-35|\n",
            "|     13| 47|     M|     educator|   29206|    36-50|\n",
            "|     14| 45|     M|    scientist|   55106|    36-50|\n",
            "|     15| 49|     F|     educator|   97301|    36-50|\n",
            "|     16| 21|     M|entertainment|   10309|    18-25|\n",
            "|     17| 30|     M|   programmer|   06355|    26-35|\n",
            "|     18| 35|     F|        other|   37212|    26-35|\n",
            "|     19| 40|     M|    librarian|   02138|    36-50|\n",
            "|     20| 42|     F|    homemaker|   95660|    36-50|\n",
            "+-------+---+------+-------------+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col,when\n",
        "\n",
        "#Adding a new column called 'age_group'\n",
        "age_grp = occupation.withColumn(\"age_group\",\n",
        "                      when((occupation.age).between(18,25), \"18-25\")\n",
        "                      .when((occupation.age).between(26,35), \"26-35\")\n",
        "                      .when((occupation.age).between(36,50), \"36-50\")\n",
        "                      .otherwise(\"51+\")\n",
        "                      )\n",
        "\n",
        "age_grp.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 5: Creating DataFrames and Converting to Spark Types\n",
        "Problem: Given the provided code snippet, create a DataFrame df using the given data and schema. The schema includes columns for firstname, middlename, lastname, id, gender, and salary. After creating the DataFrame, print its schema and display its content without truncation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XRk0jYf2OdmJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/04 15:45:42 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def create_session():\n",
        "  spk = SparkSession.builder \\\n",
        "      .master(\"local\") \\\n",
        "      .appName(\"dataframe_building\") \\\n",
        "      .getOrCreate()\n",
        "  return spk\n",
        "\n",
        "spark1 = create_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgRnFyy7EV1x",
        "outputId": "cda2fd04-fd21-4ff6-8f0c-1873f6b29b9b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+-----+------+------+\n",
            "|firstname|middlename|lastname|   id|gender|salary|\n",
            "+---------+----------+--------+-----+------+------+\n",
            "|    james|          |   smith|36636|     M|  3000|\n",
            "|  micheal|      rose|        |40288|     M|  4000|\n",
            "|   robert|          |williams|42114|     M|  4000|\n",
            "|    maria|      anne|   jones|39192|     F|  4000|\n",
            "|      jen|      mary|   brown|     |     F|    -1|\n",
            "+---------+----------+--------+-----+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark1.createDataFrame([\n",
        "    (\"james\",\"\",\"smith\",\"36636\",\"M\",3000),\n",
        "    (\"micheal\",\"rose\",\"\",\"40288\",\"M\",4000),\n",
        "    (\"robert\",\"\",\"williams\",\"42114\",\"M\",4000),\n",
        "    (\"maria\",\"anne\",\"jones\",\"39192\",\"F\",4000),\n",
        "    (\"jen\",\"mary\",\"brown\",\"\",\"F\",-1),\n",
        "],[\"firstname\",\"middlename\",\"lastname\",\"id\",\"gender\",\"salary\"])\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7oPVaSSEcKc",
        "outputId": "62646c4f-6fb1-4e22-a7da-7ecff099adb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d6HTbjkOdmJ"
      },
      "source": [
        "### Problem 6: Adding and Renaming Columns\n",
        "Problem: Add a new column \"gender\" to the existing DataFrame and rename the \"Age\" column to \"Years\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeD1r6JDOdmJ",
        "outputId": "06ed2afd-f01b-43a0-8637-e0d418569895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----+-------+-------------+--------+\n",
            "|user_id|Years| gender|   occupation|zip_code|\n",
            "+-------+-----+-------+-------------+--------+\n",
            "|      1|   24|unknown|   technician|   85711|\n",
            "|      2|   53|unknown|        other|   94043|\n",
            "|      3|   23|unknown|       writer|   32067|\n",
            "|      4|   24|unknown|   technician|   43537|\n",
            "|      5|   33|unknown|        other|   15213|\n",
            "|      6|   42|unknown|    executive|   98101|\n",
            "|      7|   57|unknown|administrator|   91344|\n",
            "|      8|   36|unknown|administrator|   05201|\n",
            "|      9|   29|unknown|      student|   01002|\n",
            "|     10|   53|unknown|       lawyer|   90703|\n",
            "|     11|   39|unknown|        other|   30329|\n",
            "|     12|   28|unknown|        other|   06405|\n",
            "|     13|   47|unknown|     educator|   29206|\n",
            "|     14|   45|unknown|    scientist|   55106|\n",
            "|     15|   49|unknown|     educator|   97301|\n",
            "|     16|   21|unknown|entertainment|   10309|\n",
            "|     17|   30|unknown|   programmer|   06355|\n",
            "|     18|   35|unknown|        other|   37212|\n",
            "|     19|   40|unknown|    librarian|   02138|\n",
            "|     20|   42|unknown|    homemaker|   95660|\n",
            "+-------+-----+-------+-------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lit,col\n",
        "\n",
        "unknown_gender = occupation.withColumn(\"gender\",lit(\"unknown\"))\n",
        "check = unknown_gender.withColumnRenamed(\"age\",\"Years\")\n",
        "check.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4fzgwd_OdmK"
      },
      "source": [
        "### Problem 7: Filtering Rows and Sorting\n",
        "Problem: Filter out users who are younger than 30 years and sort the DataFrame by age in descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjZiTAGKOdmK",
        "outputId": "571b8eba-b368-4e36-9fa6-d1c9aed1c947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----+-------+-------------+--------+\n",
            "|user_id|Years| gender|   occupation|zip_code|\n",
            "+-------+-----+-------+-------------+--------+\n",
            "|    481|   73|unknown|      retired|   37771|\n",
            "|    767|   70|unknown|     engineer|   00000|\n",
            "|    803|   70|unknown|administrator|   78212|\n",
            "|    860|   70|unknown|      retired|   48322|\n",
            "|    559|   69|unknown|    executive|   10022|\n",
            "|    585|   69|unknown|    librarian|   98501|\n",
            "|    349|   68|unknown|      retired|   61455|\n",
            "|    573|   68|unknown|      retired|   48911|\n",
            "|    211|   66|unknown|     salesman|   32605|\n",
            "|    318|   65|unknown|      retired|   06518|\n",
            "|    564|   65|unknown|      retired|   94591|\n",
            "|    651|   65|unknown|      retired|   02903|\n",
            "|    423|   64|unknown|        other|   91606|\n",
            "|    845|   64|unknown|       doctor|   97405|\n",
            "|    364|   63|unknown|     engineer|   01810|\n",
            "|    777|   63|unknown|   programmer|   01810|\n",
            "|    858|   63|unknown|     educator|   09645|\n",
            "|    266|   62|unknown|administrator|   78756|\n",
            "|    520|   62|unknown|   healthcare|   12603|\n",
            "|    106|   61|unknown|      retired|   55125|\n",
            "+-------+-----+-------+-------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Filtering out users who are younger than 30 years\n",
        "\n",
        "filtered_task = check.filter(occupation.age>30)\n",
        "sorted_result = filtered_task.orderBy(occupation.age.desc())\n",
        "sorted_result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8aHnnCMOdmK"
      },
      "source": [
        "### Problem 8: Repartitioning and Collecting Rows\n",
        "Problem: Repartition the DataFrame into 2 partitions without shuffling the data, then collect and display all rows in the driver and print number of partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "za1176toOdmK",
        "outputId": "6103c38b-fc37-4223-b980-8215e86ba442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row(firstname='james', middlename='', lastname='smith', id='36636', gender='M', salary=3000)\n",
            "Row(firstname='micheal', middlename='rose', lastname='', id='40288', gender='M', salary=4000)\n",
            "Row(firstname='robert', middlename='', lastname='williams', id='42114', gender='M', salary=4000)\n",
            "Row(firstname='maria', middlename='anne', lastname='jones', id='39192', gender='F', salary=4000)\n",
            "Row(firstname='jen', middlename='mary', lastname='brown', id='', gender='F', salary=-1)\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# Repartition the DataFrame into 2 partitions without shuffling\n",
        "repartitioned_df = df.coalesce(2)\n",
        "\n",
        "# Collect and display all rows in the driver\n",
        "all_rows = repartitioned_df.collect()\n",
        "for row in all_rows:\n",
        "    print(row)\n",
        "\n",
        "# Get the number of partitions\n",
        "num_partitions = repartitioned_df.rdd.getNumPartitions()\n",
        "print(num_partitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah2OTs_nOdmK",
        "outputId": "874572df-a15a-434b-afcb-6bd18c478b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row(firstname='james', middlename='', lastname='smith', id='36636', gender='M', salary=3000)\n",
            "Row(firstname='micheal', middlename='rose', lastname='', id='40288', gender='M', salary=4000)\n",
            "Row(firstname='robert', middlename='', lastname='williams', id='42114', gender='M', salary=4000)\n",
            "Row(firstname='maria', middlename='anne', lastname='jones', id='39192', gender='F', salary=4000)\n",
            "Row(firstname='jen', middlename='mary', lastname='brown', id='', gender='F', salary=-1)\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# Repartition the DataFrame into 2 partitions without shuffling\n",
        "repartitioned_df = df.coalesce(2)\n",
        "\n",
        "# Collect and display all rows in the driver\n",
        "all_rows = repartitioned_df.collect()\n",
        "for row in all_rows:\n",
        "    print(row)\n",
        "\n",
        "# Get the number of partitions\n",
        "num_partitions = repartitioned_df.rdd.getNumPartitions()\n",
        "print(num_partitions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzn_BGt_PuYH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFKQzCADOdmK"
      },
      "source": [
        "### Additional questions:\n",
        "\n",
        "Use both spark SQL and Pyspark to obtain answer wherever relevant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCJtYZAHS5hi"
      },
      "source": [
        "#### Filter out rows where the age is greater than 30 and create a new DataFrame. Then, add a new column named \"is_elderly\" with a value of \"True\" for these rows and \"False\" otherwise.Rename the \"gender\" column to \"sex\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPcoDeIgQ1y8",
        "outputId": "692b1c51-92f5-45bb-ad4f-8b11cbaa5fb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+------+-------------+--------+----------+---+\n",
            "|user_id|age|gender|   occupation|zip_code|is_elderly|sex|\n",
            "+-------+---+------+-------------+--------+----------+---+\n",
            "|      2| 53|     F|        other|   94043|      true|  F|\n",
            "|      5| 33|     F|        other|   15213|      true|  F|\n",
            "|      6| 42|     M|    executive|   98101|      true|  M|\n",
            "|      7| 57|     M|administrator|   91344|      true|  M|\n",
            "|      8| 36|     M|administrator|   05201|      true|  M|\n",
            "|     10| 53|     M|       lawyer|   90703|      true|  M|\n",
            "|     11| 39|     F|        other|   30329|      true|  F|\n",
            "|     13| 47|     M|     educator|   29206|      true|  M|\n",
            "|     14| 45|     M|    scientist|   55106|      true|  M|\n",
            "|     15| 49|     F|     educator|   97301|      true|  F|\n",
            "|     18| 35|     F|        other|   37212|      true|  F|\n",
            "|     19| 40|     M|    librarian|   02138|      true|  M|\n",
            "|     20| 42|     F|    homemaker|   95660|      true|  F|\n",
            "|     25| 39|     M|     engineer|   55107|      true|  M|\n",
            "|     26| 49|     M|     engineer|   21044|      true|  M|\n",
            "|     27| 40|     F|    librarian|   30030|      true|  F|\n",
            "|     28| 32|     M|       writer|   55369|      true|  M|\n",
            "|     29| 41|     M|   programmer|   94043|      true|  M|\n",
            "|     34| 38|     F|administrator|   42141|      true|  F|\n",
            "|     39| 41|     M|entertainment|   01040|      true|  M|\n",
            "+-------+---+------+-------------+--------+----------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Spark SQL\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"SparkSQLExample\").getOrCreate()\n",
        "\n",
        "# Load your data into a DataFrame (replace 'your_data_path' with the actual path)\n",
        "data = spark.read.csv('/content/occupation.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Create a temporary view to use Spark SQL\n",
        "data.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# Use Spark SQL to filter and transform the data\n",
        "filtered_data_sql = spark.sql(\"\"\"\n",
        "    SELECT *,\n",
        "           CASE WHEN age > 30 THEN True ELSE False END as is_elderly,\n",
        "           gender as sex\n",
        "    FROM people\n",
        "    WHERE age > 30\n",
        "\"\"\")\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "filtered_data_sql.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1Mr79WeQvzS"
      },
      "outputs": [],
      "source": [
        "# Pyspark\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoYi3_q7TCA9"
      },
      "source": [
        "#### Calculate the average age of male and female users separately. Present the result in a new DataFrame with columns \"gender\" and \"avg_age\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Seri4fe5Q2Ti"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/04 15:49:33 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------------------+\n",
            "|gender|           avg_age|\n",
            "+------+------------------+\n",
            "|  Male|27.333333333333332|\n",
            "|Female|              29.0|\n",
            "+------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Spark SQL\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"GenderAvgAge\").getOrCreate()\n",
        "\n",
        "# Sample user data DataFrame (replace this with your actual data)\n",
        "data = [\n",
        "    (\"Male\", 25),\n",
        "    (\"Female\", 30),\n",
        "    (\"Male\", 22),\n",
        "    (\"Female\", 28),\n",
        "    (\"Male\", 35)\n",
        "]\n",
        "\n",
        "# Create a DataFrame\n",
        "user_data = spark.createDataFrame(data, [\"gender\", \"age\"])\n",
        "\n",
        "# Register the DataFrame as a temporary SQL table\n",
        "user_data.createOrReplaceTempView(\"user_data\")\n",
        "\n",
        "# Calculate the average age for each gender using SQL\n",
        "average_age_query = \"\"\"\n",
        "    SELECT gender, AVG(age) as avg_age\n",
        "    FROM user_data\n",
        "    GROUP BY gender\n",
        "\"\"\"\n",
        "\n",
        "# Execute the SQL query\n",
        "average_age_result = spark.sql(average_age_query)\n",
        "\n",
        "# Show the result\n",
        "average_age_result.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "433nZ-6lQv5F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   gender    avg_age\n",
            "0  Female  29.000000\n",
            "1    Male  27.333333\n"
          ]
        }
      ],
      "source": [
        "# Pyspark\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Sample user_data DataFrame (replace this with your actual data)\n",
        "data = {\n",
        "    'gender': ['Male', 'Female', 'Male', 'Female', 'Male'],\n",
        "    'age': [25, 30, 22, 28, 35]\n",
        "}\n",
        "\n",
        "user_data = pd.DataFrame(data)\n",
        "\n",
        "# Calculate average age for each gender\n",
        "average_age_by_gender = user_data.groupby('gender')['age'].mean().reset_index()\n",
        "\n",
        "# Rename the columns\n",
        "average_age_by_gender.columns = ['gender', 'avg_age']\n",
        "\n",
        "# Display the result\n",
        "print(average_age_by_gender)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyMs561GTKtI"
      },
      "source": [
        "#### Add a new column named \"full_name\" to the dataset by concatenating the \"user_id\" and \"occupation\" columns. Then, rename the \"zip_code\" column to \"postal_code\" in the same DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sZc6kifIQ2qa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/04 15:50:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+---------+----------+-----------+------------+\n",
            "|user_id|first_name|last_name|occupation|postal_code|   full_name|\n",
            "+-------+----------+---------+----------+-----------+------------+\n",
            "|      1|      John|      Doe|  Engineer|      12345|1 - Engineer|\n",
            "|      2|      Jane|    Smith|   Teacher|      54321| 2 - Teacher|\n",
            "|      3|     Alice|  Johnson|    Doctor|      67890|  3 - Doctor|\n",
            "+-------+----------+---------+----------+-----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Spark SQL\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat_ws, col\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"AddColumnAndRename\").getOrCreate()\n",
        "\n",
        "# Sample user data DataFrame (replace this with your actual data)\n",
        "data = [\n",
        "    (1, \"John\", \"Doe\", \"Engineer\", \"12345\"),\n",
        "    (2, \"Jane\", \"Smith\", \"Teacher\", \"54321\"),\n",
        "    (3, \"Alice\", \"Johnson\", \"Doctor\", \"67890\"),\n",
        "]\n",
        "\n",
        "# Create a DataFrame\n",
        "user_data = spark.createDataFrame(data, [\"user_id\", \"first_name\", \"last_name\", \"occupation\", \"zip_code\"])\n",
        "\n",
        "# Add a new column \"full_name\" by concatenating \"user_id\" and \"occupation\"\n",
        "user_data = user_data.withColumn(\"full_name\", concat_ws(\" - \", col(\"user_id\"), col(\"occupation\")))\n",
        "\n",
        "# Rename the \"zip_code\" column to \"postal_code\"\n",
        "user_data = user_data.withColumnRenamed(\"zip_code\", \"postal_code\")\n",
        "\n",
        "# Show the updated DataFrame\n",
        "user_data.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hC_1VJbnQwFI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+---------+----------+-----------+------------+\n",
            "|user_id|first_name|last_name|occupation|postal_code|   full_name|\n",
            "+-------+----------+---------+----------+-----------+------------+\n",
            "|      1|      John|      Doe|  Engineer|      12345|1 - Engineer|\n",
            "|      2|      Jane|    Smith|   Teacher|      54321| 2 - Teacher|\n",
            "|      3|     Alice|  Johnson|    Doctor|      67890|  3 - Doctor|\n",
            "+-------+----------+---------+----------+-----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat_ws, col\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"AddColumnAndRename\").getOrCreate()\n",
        "\n",
        "# Sample user data DataFrame (replace this with your actual data)\n",
        "data = [\n",
        "    (1, \"John\", \"Doe\", \"Engineer\", \"12345\"),\n",
        "    (2, \"Jane\", \"Smith\", \"Teacher\", \"54321\"),\n",
        "    (3, \"Alice\", \"Johnson\", \"Doctor\", \"67890\"),\n",
        "]\n",
        "\n",
        "# Create a DataFrame\n",
        "user_data = spark.createDataFrame(data, [\"user_id\", \"first_name\", \"last_name\", \"occupation\", \"zip_code\"])\n",
        "\n",
        "# Add a new column \"full_name\" by concatenating \"user_id\" and \"occupation\"\n",
        "user_data = user_data.withColumn(\"full_name\", concat_ws(\" - \", col(\"user_id\"), col(\"occupation\")))\n",
        "\n",
        "# Rename the \"zip_code\" column to \"postal_code\"\n",
        "user_data = user_data.withColumnRenamed(\"zip_code\", \"postal_code\")\n",
        "\n",
        "# Show the updated DataFrame\n",
        "user_data.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CPDOqPZUTWd"
      },
      "source": [
        "#### Filter out rows where occupation is 'technician', select only the \"user_id\" and \"age\" columns, and then add a new column \"age_diff\" that calculates the difference between the user's age and the average age in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8-rf3eAUZ0q"
      },
      "outputs": [],
      "source": [
        "# Spark SQL\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HJ1PEFNUbg0"
      },
      "outputs": [],
      "source": [
        "# Pyspark\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTIR2tJuTRID"
      },
      "source": [
        "#### Divide the dataset into two DataFrames: one with male users and another with female users. Repartition both DataFrames to have 2 partitions each. Then, union these two DataFrames together and display the resulting DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4JKAn2n0P6Ib"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/04 15:52:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+---------+------+----------+\n",
            "|user_id|first_name|last_name|gender|occupation|\n",
            "+-------+----------+---------+------+----------+\n",
            "|      1|      John|      Doe|  Male|  Engineer|\n",
            "|      4|       Bob|    Brown|  Male|    Artist|\n",
            "|      2|      Jane|    Smith|Female|   Teacher|\n",
            "|      3|     Alice|  Johnson|Female|    Doctor|\n",
            "+-------+----------+---------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"GenderDivideRepartitionUnion\").getOrCreate()\n",
        "\n",
        "# Sample user data DataFrame (replace this with your actual data)\n",
        "data = [\n",
        "    (1, \"John\", \"Doe\", \"Male\", \"Engineer\"),\n",
        "    (2, \"Jane\", \"Smith\", \"Female\", \"Teacher\"),\n",
        "    (3, \"Alice\", \"Johnson\", \"Female\", \"Doctor\"),\n",
        "    (4, \"Bob\", \"Brown\", \"Male\", \"Artist\"),\n",
        "]\n",
        "\n",
        "# Create a DataFrame\n",
        "user_data = spark.createDataFrame(data, [\"user_id\", \"first_name\", \"last_name\", \"gender\", \"occupation\"])\n",
        "\n",
        "# Divide the dataset into two DataFrames: one with male users and one with female users\n",
        "male_users = user_data.filter(col(\"gender\") == \"Male\")\n",
        "female_users = user_data.filter(col(\"gender\") == \"Female\")\n",
        "\n",
        "# Repartition both DataFrames to have 2 partitions each\n",
        "male_users = male_users.repartition(2)\n",
        "female_users = female_users.repartition(2)\n",
        "\n",
        "# Union the two DataFrames\n",
        "combined_users = male_users.union(female_users)\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "combined_users.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfVdjYOcTn_g"
      },
      "source": [
        "#### Create and fill a new DataFrame named user_ratings with columns user_id and rating max 10 column. Both user_data and user_ratings share the user_id column. Combine these two DataFrames to create a new DataFrame that includes user information and their corresponding ratings. Make sure to keep only the users present in both DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "a3Jw-vaEP4U4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/04 15:51:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+---------+----------+-------------+\n",
            "|user_id|first_name|last_name|occupation|rating_max_10|\n",
            "+-------+----------+---------+----------+-------------+\n",
            "|      1|      John|      Doe|  Engineer|            8|\n",
            "|      2|      Jane|    Smith|   Teacher|            9|\n",
            "|      3|     Alice|  Johnson|    Doctor|           10|\n",
            "+-------+----------+---------+----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"UserRatings\").getOrCreate()\n",
        "\n",
        "# Sample user data DataFrame (replace this with your actual data)\n",
        "user_data_data = [\n",
        "    (1, \"John\", \"Doe\", \"Engineer\"),\n",
        "    (2, \"Jane\", \"Smith\", \"Teacher\"),\n",
        "    (3, \"Alice\", \"Johnson\", \"Doctor\"),\n",
        "]\n",
        "\n",
        "# Create a user data DataFrame\n",
        "user_data = spark.createDataFrame(user_data_data, [\"user_id\", \"first_name\", \"last_name\", \"occupation\"])\n",
        "\n",
        "# Sample user ratings DataFrame (replace this with your actual data)\n",
        "user_ratings_data = [\n",
        "    (1, 8),\n",
        "    (2, 9),\n",
        "    (4, 7),  # User not present in user_data\n",
        "    (3, 10),\n",
        "]\n",
        "\n",
        "# Create a user ratings DataFrame\n",
        "user_ratings = spark.createDataFrame(user_ratings_data, [\"user_id\", \"rating_max_10\"])\n",
        "\n",
        "# Combine user_data and user_ratings DataFrames for users present in both\n",
        "combined_data = user_data.join(user_ratings, \"user_id\", \"inner\")\n",
        "\n",
        "# Show the combined DataFrame\n",
        "combined_data.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5 (main, Aug 24 2023, 15:09:45) [Clang 14.0.3 (clang-1403.0.22.14.1)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
